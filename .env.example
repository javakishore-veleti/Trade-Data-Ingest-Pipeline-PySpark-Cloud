# Common
SPARK_APP_NAME=TradeDataIngest

# Environment context
CLOUD_PROVIDER=local  # options: local, emr, databricks, azure, gcp

# Optional: Spark master override
SPARK_MASTER_URL=local[*]  # Override for cluster-based: yarn, spark://..., etc.

SPARK_EXTRA_JARS=./lib/postgresql-42.6.0.jar

POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=tradedb
POSTGRES_USER=postgres
POSTGRES_PASSWORD=admin
POSTGRES_TABLE=Trade_Event

# AWS EMR
AWS_ACCESS_KEY_ID=your-access-key
AWS_SECRET_ACCESS_KEY=your-secret
AWS_REGION=us-east-1
S3_INPUT_PATH=s3://your-bucket/trades/
JARS_S3_PATH=s3://your-bucket/jars/postgresql-42.6.0.jar

# Azure ADLS
AZURE_STORAGE_ACCOUNT=myaccount
AZURE_STORAGE_KEY=secretkey
AZURE_CLIENT_ID=...
AZURE_CLIENT_SECRET=...
AZURE_TENANT_ID=...
AZURE_CONTAINER=mycontainer
AZURE_JAR_PATH=abfss://<your-path>/postgresql-42.6.0.jar

# Databricks (in-cluster, no setup needed usually)
